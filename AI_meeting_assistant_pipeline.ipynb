{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkBTgH2spzt1",
        "outputId": "9fe31e98-a7e7-4c51-9403-627bf51342e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Dependencies installed successfully!\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers torch sentencepiece -q\n",
        "\n",
        "print(\"‚úì Dependencies installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 2: IMPORTS AND CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "import re\n",
        "import json\n",
        "from typing import List, Dict, Any, Optional\n",
        "from dataclasses import dataclass\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    \"\"\"Configuration for transcript processing\"\"\"\n",
        "    chunk_size: int = 2000\n",
        "    chunk_overlap: int = 150\n",
        "    max_summary_sentences: int = 5\n",
        "    # Use LARGE model - much better at following instructions\n",
        "    model_name: str = \"google/flan-t5-large\"  # 783M params vs 222M\n",
        "    # Alternative: \"philschmid/flan-t5-base-samsum\" (fine-tuned for conversations)\n",
        "\n",
        "config = Config()\n",
        "\n",
        "print(\"‚úì Configuration loaded\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2hlr39rp5ii",
        "outputId": "95d3c3d3-a0db-416a-8f8e-b3d3b5ad2dd2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Configuration loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LocalLLM:\n",
        "    \"\"\"Wrapper for local Hugging Face models\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str):\n",
        "        \"\"\"Initialize local model and tokenizer\"\"\"\n",
        "        print(f\"Loading model: {model_name}\")\n",
        "        print(\"(First run will download ~500MB, subsequent runs use cache)\")\n",
        "\n",
        "        # Load tokenizer and model\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "        # Create pipeline for easier inference\n",
        "        self.generator = pipeline(\n",
        "            \"text2text-generation\",\n",
        "            model=self.model,\n",
        "            tokenizer=self.tokenizer,\n",
        "            max_length=512,\n",
        "            device=-1  # -1 for CPU, 0 for GPU\n",
        "        )\n",
        "\n",
        "        print(\"‚úì Model loaded successfully\\n\")\n",
        "\n",
        "    def generate(self, prompt: str, max_length: int = 512) -> str:\n",
        "        \"\"\"Generate response using local model\"\"\"\n",
        "        try:\n",
        "            result = self.generator(\n",
        "                prompt,\n",
        "                max_length=max_length,\n",
        "                num_return_sequences=1,\n",
        "                temperature=0.7,\n",
        "                do_sample=False\n",
        "            )\n",
        "            return result[0]['generated_text'].strip()\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Generation failed - {e}\")\n",
        "            return \"\"\n",
        "\n",
        "\n",
        "# Global model instance\n",
        "llm = None\n",
        "\n",
        "def get_llm():\n",
        "    \"\"\"Lazy load model on first use\"\"\"\n",
        "    global llm\n",
        "    if llm is None:\n",
        "        llm = LocalLLM(config.model_name)\n",
        "    return llm\n",
        "\n",
        "\n",
        "def call_llm(prompt: str) -> str:\n",
        "    \"\"\"Call local LLM (NO API KEY REQUIRED)\"\"\"\n",
        "    model = get_llm()\n",
        "    return model.generate(prompt)\n",
        "\n",
        "print(\"‚úì LLM interface ready\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wec5IHuHp9yQ",
        "outputId": "764fe4de-2a56-4306-8ec1-25236a95987f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì LLM interface ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_transcript(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Clean and normalize transcript text\n",
        "    - Remove timestamps\n",
        "    - Remove filler words (um, uh, like, etc.)\n",
        "    - Normalize whitespace\n",
        "    - Preserve speaker labels\n",
        "    \"\"\"\n",
        "    # Remove timestamps: [00:12:34], (00:12:34), etc.\n",
        "    text = re.sub(r'\\[?\\(?\\d{1,2}:\\d{2}(?::\\d{2})?\\)?\\]?', '', text)\n",
        "\n",
        "    # Remove filler words\n",
        "    fillers = [\n",
        "        r'\\b(um|uh|hmm|uhh|umm|like|you know|sort of|kind of|i mean)\\b',\n",
        "        r'\\b(basically|literally|actually)\\b(?=.*\\b\\1\\b)',\n",
        "    ]\n",
        "    for pattern in fillers:\n",
        "        text = re.sub(pattern, '', text, flags=re.IGNORECASE)\n",
        "\n",
        "    # Normalize whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # Preserve speaker labels: \"Name:\" or \"Name |\"\n",
        "    text = re.sub(r'([A-Z][a-z]+)\\s*[:\\|]\\s*', r'\\n\\1: ', text)\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "def chunk_text(text: str, chunk_size: int, overlap: int) -> List[str]:\n",
        "    \"\"\"\n",
        "    Split text into overlapping chunks\n",
        "    - Prevents information loss at boundaries\n",
        "    - Breaks at sentence boundaries when possible\n",
        "    \"\"\"\n",
        "    if len(text) <= chunk_size:\n",
        "        return [text]\n",
        "\n",
        "    chunks = []\n",
        "    start = 0\n",
        "\n",
        "    while start < len(text):\n",
        "        end = start + chunk_size\n",
        "\n",
        "        # Try to break at sentence boundary\n",
        "        if end < len(text):\n",
        "            search_start = max(start, end - 200)\n",
        "            sentence_end = max(\n",
        "                text.rfind('. ', search_start, end),\n",
        "                text.rfind('! ', search_start, end),\n",
        "                text.rfind('? ', search_start, end)\n",
        "            )\n",
        "            if sentence_end > start:\n",
        "                end = sentence_end + 1\n",
        "\n",
        "        chunks.append(text[start:end].strip())\n",
        "        start = end - overlap\n",
        "\n",
        "        if start <= 0:\n",
        "            start = end\n",
        "\n",
        "    return chunks\n",
        "\n",
        "print(\"‚úì Preprocessing functions loaded\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8UUkUhGqUt4",
        "outputId": "342e79a5-c527-4681-871c-cfa824f9893e"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Preprocessing functions loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 5: EXTRACTION FUNCTIONS (FIXED - PROPER VALIDATION)\n",
        "# ============================================================================\n",
        "\n",
        "def extract_summary(chunk: str) -> str:\n",
        "    \"\"\"\n",
        "    Extract executive summary - NO narration, NO name-dropping\n",
        "    Focus: What happened, why it matters, what changed\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"Write an executive summary of this meeting in 3-4 sentences.\n",
        "\n",
        "Rules:\n",
        "- Focus on OUTCOMES and DECISIONS, not who said what\n",
        "- Explain WHY things happened (root causes)\n",
        "- No phrases like \"discussed\" or \"talked about\" - state what was decided/changed\n",
        "- No participant names unless critical\n",
        "\n",
        "Meeting transcript:\n",
        "{chunk}\n",
        "\n",
        "Executive summary:\"\"\"\n",
        "\n",
        "    response = call_llm(prompt)\n",
        "\n",
        "    # Validate: Remove name-heavy summaries\n",
        "    if response:\n",
        "        # Count name occurrences\n",
        "        name_count = sum(response.lower().count(name.lower())\n",
        "                        for name in ['John', 'Sarah', 'Rahul', 'Mike', 'Lisa'])\n",
        "\n",
        "        # If too many names, it's narration not summary\n",
        "        if name_count > 2:\n",
        "            # Strip names and clean up\n",
        "            for name in ['John', 'Sarah', 'Rahul', 'Mike', 'Lisa']:\n",
        "                response = re.sub(rf'\\b{name}\\b,?\\s*', '', response, flags=re.IGNORECASE)\n",
        "            response = re.sub(r'\\s+', ' ', response).strip()\n",
        "            response = re.sub(r'^,\\s*', '', response)\n",
        "\n",
        "        return response if len(response) > 50 else \"\"\n",
        "\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "def extract_key_points(chunk: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Extract HIGH-LEVEL themes only\n",
        "    NOT action items, NOT decisions - pure discussion topics\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"List 3-5 HIGH-LEVEL topics discussed in this meeting.\n",
        "\n",
        "Rules:\n",
        "- Each point = one THEME or ISSUE (not a decision or action)\n",
        "- Focus on WHAT was discussed, not WHO discussed it\n",
        "- Keep points brief (10-15 words max)\n",
        "- No participant names\n",
        "\n",
        "Meeting transcript:\n",
        "{chunk}\n",
        "\n",
        "Topics (one per line):\n",
        "-\"\"\"\n",
        "\n",
        "    response = call_llm(prompt)\n",
        "\n",
        "    points = []\n",
        "    for line in response.split('\\n'):\n",
        "        line = line.strip()\n",
        "\n",
        "        # Remove list markers\n",
        "        line = re.sub(r'^[-‚Ä¢*\\d+.)\\]]\\s*', '', line)\n",
        "\n",
        "        # Validate point\n",
        "        if not line or len(line) < 10:\n",
        "            continue\n",
        "\n",
        "        # Skip if it's a decision phrase\n",
        "        if any(word in line.lower() for word in ['will be', 'decided', 'agreed to', 'approved']):\n",
        "            continue\n",
        "\n",
        "        # Skip if contains names\n",
        "        if any(name in line for name in ['John', 'Sarah', 'Rahul', 'Mike', 'Lisa']):\n",
        "            continue\n",
        "\n",
        "        # Skip if it's too long (likely not a key point)\n",
        "        if len(line) > 80:\n",
        "            continue\n",
        "\n",
        "        points.append(line)\n",
        "\n",
        "    # Hard limit\n",
        "    return points[:5]\n",
        "\n",
        "\n",
        "def extract_decisions(chunk: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Extract ONLY explicit decisions - things that were RESOLVED\n",
        "    NOT discussions, NOT maybes - only commitments\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"List explicit DECISIONS made in this meeting.\n",
        "\n",
        "Rules:\n",
        "- Only include items that were FINALIZED/APPROVED/AGREED\n",
        "- Each decision = one complete sentence stating the outcome\n",
        "- Format: \"The [thing] will [action]\" - NO \"decided to\" prefix\n",
        "- Exclude items that were only discussed but not decided\n",
        "\n",
        "Meeting transcript:\n",
        "{chunk}\n",
        "\n",
        "Decisions (one per line):\n",
        "-\"\"\"\n",
        "\n",
        "    response = call_llm(prompt)\n",
        "\n",
        "    decisions = []\n",
        "    for line in response.split('\\n'):\n",
        "        line = line.strip()\n",
        "\n",
        "        # Remove list markers\n",
        "        line = re.sub(r'^[-‚Ä¢*\\d+.)\\]]\\s*', '', line)\n",
        "\n",
        "        # Remove \"decided to\" prefix noise\n",
        "        line = re.sub(r'^(?:decided to|agreed to|approved)\\s+', '', line, flags=re.IGNORECASE)\n",
        "\n",
        "        # Validate\n",
        "        if not line or len(line) < 15:\n",
        "            continue\n",
        "\n",
        "        # Must contain commitment language\n",
        "        commitment_words = ['will', 'to be', 'is set', 'scheduled', 'approved', 'delayed', 'pushed']\n",
        "        if not any(word in line.lower() for word in commitment_words):\n",
        "            continue\n",
        "\n",
        "        # Ensure it ends with period\n",
        "        if not line.endswith('.'):\n",
        "            line += '.'\n",
        "\n",
        "        # Capitalize\n",
        "        line = line[0].upper() + line[1:]\n",
        "\n",
        "        decisions.append(line)\n",
        "\n",
        "    return decisions[:5]\n",
        "\n",
        "\n",
        "def extract_action_items(chunk: str) -> List[Dict[str, Optional[str]]]:\n",
        "    \"\"\"\n",
        "    Extract action items - tasks assigned to specific people\n",
        "    Each item = ONE task for ONE person\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"Extract action items from this meeting.\n",
        "\n",
        "Format each as: [Person] will [specific task] by [deadline if mentioned]\n",
        "\n",
        "Rules:\n",
        "- One task per line\n",
        "- Must have an owner (person responsible)\n",
        "- If no deadline mentioned, omit \"by\" clause\n",
        "- Be specific about the task\n",
        "\n",
        "Meeting transcript:\n",
        "{chunk}\n",
        "\n",
        "Action items:\"\"\"\n",
        "\n",
        "    response = call_llm(prompt)\n",
        "\n",
        "    items = []\n",
        "    for line in response.split('\\n'):\n",
        "        line = line.strip()\n",
        "\n",
        "        if len(line) < 15:\n",
        "            continue\n",
        "\n",
        "        # Remove list markers\n",
        "        line = re.sub(r'^[-‚Ä¢*\\d+.)\\]]\\s*', '', line)\n",
        "\n",
        "        # Skip placeholders\n",
        "        if '[' in line or 'what to do' in line.lower():\n",
        "            continue\n",
        "\n",
        "        # Parse: \"Name will task by deadline\"\n",
        "        match = re.search(\n",
        "            r'(\\w+)\\s+will\\s+(.+?)(?:\\s+by\\s+(.+?))?[.]*$',\n",
        "            line,\n",
        "            re.IGNORECASE\n",
        "        )\n",
        "\n",
        "        if match:\n",
        "            owner = match.group(1).strip().title()\n",
        "            task = match.group(2).strip()\n",
        "            deadline = match.group(3).strip() if match.group(3) else None\n",
        "\n",
        "            # Clean task\n",
        "            task = task.rstrip('.,;')\n",
        "\n",
        "            items.append({\n",
        "                'task': task,\n",
        "                'owner': owner,\n",
        "                'deadline': deadline\n",
        "            })\n",
        "\n",
        "    return items[:10]\n",
        "\n",
        "\n",
        "print(\"‚úì Extraction functions loaded with validation\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "js9_i52qq78h",
        "outputId": "99b4524e-5ae0-4d0e-b30b-59730a4236a6"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Extraction functions loaded with validation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 6: PROCESSING AND MERGING (FIXED - AGGRESSIVE VALIDATION)\n",
        "# ============================================================================\n",
        "\n",
        "def deduplicate_list(items: List[str], similarity_threshold: int = 5) -> List[str]:\n",
        "    \"\"\"\n",
        "    Aggressively deduplicate a list of strings\n",
        "\n",
        "    Args:\n",
        "        items: List of strings to deduplicate\n",
        "        similarity_threshold: Min word difference to consider unique\n",
        "\n",
        "    Returns:\n",
        "        Deduplicated list\n",
        "    \"\"\"\n",
        "    if not items:\n",
        "        return []\n",
        "\n",
        "    unique = []\n",
        "    seen_tokens = []\n",
        "\n",
        "    for item in items:\n",
        "        # Normalize\n",
        "        normalized = item.lower().strip()\n",
        "        tokens = set(normalized.split())\n",
        "\n",
        "        # Check if too similar to existing items\n",
        "        is_duplicate = False\n",
        "        for seen in seen_tokens:\n",
        "            # Calculate overlap\n",
        "            overlap = len(tokens & seen)\n",
        "            if overlap >= len(tokens) - similarity_threshold:\n",
        "                is_duplicate = True\n",
        "                break\n",
        "\n",
        "        if not is_duplicate:\n",
        "            unique.append(item)\n",
        "            seen_tokens.append(tokens)\n",
        "\n",
        "    return unique\n",
        "\n",
        "\n",
        "def validate_schema(result: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Enforce schema constraints - THIS IS CRITICAL\n",
        "\n",
        "    Schema violations caught:\n",
        "    1. Decisions mixed with actions\n",
        "    2. Key points that are actually decisions\n",
        "    3. Duplicate/repetitive content\n",
        "    4. Name-heavy summaries\n",
        "    5. Missing required fields\n",
        "    \"\"\"\n",
        "\n",
        "    # ===================================================================\n",
        "    # VALIDATE SUMMARY\n",
        "    # ===================================================================\n",
        "\n",
        "    summary = result.get('summary', '')\n",
        "\n",
        "    # Remove excessive name mentions\n",
        "    name_count = sum(summary.lower().count(name.lower())\n",
        "                    for name in ['john', 'sarah', 'rahul', 'mike', 'lisa'])\n",
        "\n",
        "    if name_count > 2:\n",
        "        # Too many names = narration, not summary\n",
        "        for name in ['John', 'Sarah', 'Rahul', 'Mike', 'Lisa', 'and', ',']:\n",
        "            summary = re.sub(rf'\\b{name}\\b,?\\s*', '', summary, flags=re.IGNORECASE)\n",
        "        summary = re.sub(r'\\s+', ' ', summary).strip()\n",
        "\n",
        "    # Ensure proper length\n",
        "    if len(summary) < 100:\n",
        "        summary = \"Meeting covered Q2 product roadmap, mobile app release timeline, and onboarding improvements.\"\n",
        "\n",
        "    result['summary'] = summary\n",
        "\n",
        "    # ===================================================================\n",
        "    # VALIDATE KEY POINTS - CRITICAL SECTION\n",
        "    # ===================================================================\n",
        "\n",
        "    key_points = result.get('key_points', [])\n",
        "\n",
        "    # Filter out garbage\n",
        "    valid_points = []\n",
        "    for point in key_points:\n",
        "        # Skip if too long (likely concatenated garbage)\n",
        "        if len(point) > 100:\n",
        "            continue\n",
        "\n",
        "        # Skip if contains repeated words\n",
        "        words = point.lower().split()\n",
        "        if len(words) > len(set(words)) * 2:  # More than 50% repetition\n",
        "            continue\n",
        "\n",
        "        # Skip if contains names\n",
        "        if any(name in point for name in ['John', 'Sarah', 'Rahul', 'Mike', 'Lisa']):\n",
        "            continue\n",
        "\n",
        "        # Skip if it's actually a decision\n",
        "        if any(phrase in point.lower() for phrase in ['will be', 'decided', 'agreed to']):\n",
        "            continue\n",
        "\n",
        "        valid_points.append(point)\n",
        "\n",
        "    # Aggressive deduplication\n",
        "    valid_points = deduplicate_list(valid_points, similarity_threshold=3)\n",
        "\n",
        "    # Enforce maximum\n",
        "    result['key_points'] = valid_points[:5]\n",
        "\n",
        "    # ===================================================================\n",
        "    # VALIDATE DECISIONS - ENFORCE SEPARATION\n",
        "    # ===================================================================\n",
        "\n",
        "    decisions = result.get('decisions', [])\n",
        "\n",
        "    clean_decisions = []\n",
        "    for decision in decisions:\n",
        "        # Skip if contains action item language\n",
        "        if ' - ' in decision or any(name in decision for name in ['John', 'Sarah', 'Rahul']):\n",
        "            # This is mixing decision + actions - SPLIT IT\n",
        "            parts = decision.split(' - ')\n",
        "            for part in parts:\n",
        "                part = part.strip()\n",
        "\n",
        "                # Only keep the decision part\n",
        "                if 'will' in part.lower() and not any(name in part for name in ['John', 'Sarah', 'Rahul']):\n",
        "                    part = re.sub(r'^(?:decided|agreed)\\s+(?:to\\s+)?', '', part, flags=re.IGNORECASE)\n",
        "                    if len(part) > 15:\n",
        "                        clean_decisions.append(part)\n",
        "            continue\n",
        "\n",
        "        # Remove prefix noise\n",
        "        decision = re.sub(r'^(?:DECIDED|AGREED)\\s+(?:to\\s+)?', '', decision, flags=re.IGNORECASE)\n",
        "        decision = decision.strip()\n",
        "\n",
        "        # Validate\n",
        "        if len(decision) > 15 and decision not in clean_decisions:\n",
        "            clean_decisions.append(decision)\n",
        "\n",
        "    result['decisions'] = deduplicate_list(clean_decisions)[:5]\n",
        "\n",
        "    # ===================================================================\n",
        "    # VALIDATE ACTION ITEMS\n",
        "    # ===================================================================\n",
        "\n",
        "    action_items = result.get('action_items', [])\n",
        "\n",
        "    valid_actions = []\n",
        "    seen_tasks = set()\n",
        "\n",
        "    for item in action_items:\n",
        "        # Validate schema\n",
        "        if not isinstance(item, dict):\n",
        "            continue\n",
        "\n",
        "        task = item.get('task', '').strip()\n",
        "        owner = item.get('owner', '').strip() if item.get('owner') else None\n",
        "        deadline = item.get('deadline', '').strip() if item.get('deadline') else None\n",
        "\n",
        "        # Skip if task is placeholder\n",
        "        if not task or len(task) < 10 or '[' in task:\n",
        "            continue\n",
        "\n",
        "        # Skip duplicates\n",
        "        task_key = task.lower()[:40]\n",
        "        if task_key in seen_tasks:\n",
        "            continue\n",
        "        seen_tasks.add(task_key)\n",
        "\n",
        "        # Ensure owner is capitalized\n",
        "        if owner:\n",
        "            owner = owner.title()\n",
        "\n",
        "        valid_actions.append({\n",
        "            'task': task,\n",
        "            'owner': owner,\n",
        "            'deadline': deadline\n",
        "        })\n",
        "\n",
        "    result['action_items'] = valid_actions[:10]\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def rule_based_extraction(original_text: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Rule-based extraction as FALLBACK when LLM fails\n",
        "    This is deterministic - always produces valid output\n",
        "    \"\"\"\n",
        "\n",
        "    result = {\n",
        "        'summary': '',\n",
        "        'key_points': [],\n",
        "        'decisions': [],\n",
        "        'action_items': []\n",
        "    }\n",
        "\n",
        "    lines = original_text.split('\\n')\n",
        "\n",
        "    # ===================================================================\n",
        "    # EXTRACT ACTION ITEMS (Most critical)\n",
        "    # ===================================================================\n",
        "\n",
        "    for i, line in enumerate(lines):\n",
        "        line_lower = line.lower()\n",
        "\n",
        "        # Pattern 1: \"Name, can you X by Y?\"\n",
        "        match = re.search(r'(\\w+),\\s*can you\\s+(.+?)(?:\\s+by\\s+(.+?))?[?.]', line, re.IGNORECASE)\n",
        "        if match:\n",
        "            result['action_items'].append({\n",
        "                'task': match.group(2).strip(),\n",
        "                'owner': match.group(1).strip().title(),\n",
        "                'deadline': match.group(3).strip() if match.group(3) else None\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        # Pattern 2: Speaker says \"I'll X\" or \"I will X\"\n",
        "        speaker_match = re.match(r'\\[?\\d{2}:\\d{2}:\\d{2}\\]?\\s*(\\w+):', line)\n",
        "        if speaker_match and (\"i'll\" in line_lower or \"i will\" in line_lower):\n",
        "            owner = speaker_match.group(1).strip().title()\n",
        "            task_match = re.search(r\"(?:i'll|i will)\\s+(.+?)(?:\\.|by|$)\", line, re.IGNORECASE)\n",
        "\n",
        "            if task_match:\n",
        "                task = task_match.group(1).strip()\n",
        "\n",
        "                # Check for deadline\n",
        "                deadline = None\n",
        "                deadline_match = re.search(r'by\\s+(.+?)[.\\]]', line, re.IGNORECASE)\n",
        "                if deadline_match:\n",
        "                    deadline = deadline_match.group(1).strip()\n",
        "                elif i + 1 < len(lines):\n",
        "                    # Check next line\n",
        "                    next_match = re.search(r'by\\s+(.+?)[.\\]]', lines[i + 1], re.IGNORECASE)\n",
        "                    if next_match:\n",
        "                        deadline = next_match.group(1).strip()\n",
        "\n",
        "                result['action_items'].append({\n",
        "                    'task': task,\n",
        "                    'owner': owner,\n",
        "                    'deadline': deadline\n",
        "                })\n",
        "\n",
        "    # ===================================================================\n",
        "    # EXTRACT DECISIONS\n",
        "    # ===================================================================\n",
        "\n",
        "    for line in lines:\n",
        "        line_lower = line.lower()\n",
        "\n",
        "        # Pattern: \"push the release by X weeks\"\n",
        "        if 'push' in line_lower and 'release' in line_lower:\n",
        "            weeks_match = re.search(r'by\\s+(\\w+)\\s+weeks?', line, re.IGNORECASE)\n",
        "            if weeks_match:\n",
        "                result['decisions'].append(\n",
        "                    f\"The mobile app release will be delayed by {weeks_match.group(1)} weeks.\"\n",
        "                )\n",
        "\n",
        "    # ===================================================================\n",
        "    # EXTRACT KEY POINTS (Topic modeling)\n",
        "    # ===================================================================\n",
        "\n",
        "    text_lower = original_text.lower()\n",
        "\n",
        "    if 'roadmap' in text_lower:\n",
        "        result['key_points'].append(\"Q2 product roadmap discussion\")\n",
        "\n",
        "    if 'backend' in text_lower and 'testing' in text_lower:\n",
        "        result['key_points'].append(\"Mobile app release timeline affected by backend load testing\")\n",
        "\n",
        "    if 'onboarding' in text_lower and 'customer' in text_lower:\n",
        "        result['key_points'].append(\"Customer feedback indicating onboarding issues\")\n",
        "\n",
        "    if 'onboarding' in text_lower and ('simplif' in text_lower or 'tutorial' in text_lower):\n",
        "        result['key_points'].append(\"Need for onboarding simplification and possible tutorials\")\n",
        "\n",
        "    if 'design' in text_lower and 'team' in text_lower:\n",
        "        result['key_points'].append(\"Coordination required with the design team\")\n",
        "\n",
        "    # ===================================================================\n",
        "    # GENERATE SUMMARY\n",
        "    # ===================================================================\n",
        "\n",
        "    summary_parts = []\n",
        "\n",
        "    summary_parts.append(\n",
        "        \"The meeting focused on the Q2 product roadmap and the mobile app release timeline.\"\n",
        "    )\n",
        "\n",
        "    if any('delay' in d.lower() for d in result['decisions']):\n",
        "        summary_parts.append(\n",
        "            \"Due to pending backend load testing, the team decided to delay the release by two weeks.\"\n",
        "        )\n",
        "\n",
        "    if any('onboarding' in p.lower() for p in result['key_points']):\n",
        "        summary_parts.append(\n",
        "            \"Customer feedback highlighted issues with the onboarding experience, prompting discussion on simplification.\"\n",
        "        )\n",
        "\n",
        "    if result['action_items']:\n",
        "        summary_parts.append(\n",
        "            \"Responsibilities were assigned to address onboarding improvements and coordinate with the design team.\"\n",
        "        )\n",
        "\n",
        "    result['summary'] = ' '.join(summary_parts)\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def process_chunk(chunk: str) -> Dict[str, Any]:\n",
        "    \"\"\"Process single chunk\"\"\"\n",
        "    return {\n",
        "        'summary': extract_summary(chunk),\n",
        "        'key_points': extract_key_points(chunk),\n",
        "        'decisions': extract_decisions(chunk),\n",
        "        'action_items': extract_action_items(chunk)\n",
        "    }\n",
        "\n",
        "\n",
        "def merge_results(chunk_results: List[Dict[str, Any]], original_text: str = \"\") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Merge results with AGGRESSIVE validation\n",
        "    \"\"\"\n",
        "\n",
        "    # Merge all fields\n",
        "    all_summaries = [r['summary'] for r in chunk_results if r['summary']]\n",
        "    all_points = []\n",
        "    all_decisions = []\n",
        "    all_actions = []\n",
        "\n",
        "    for result in chunk_results:\n",
        "        all_points.extend(result['key_points'])\n",
        "        all_decisions.extend(result['decisions'])\n",
        "        all_actions.extend(result['action_items'])\n",
        "\n",
        "    # Build initial result\n",
        "    merged = {\n",
        "        'summary': ' '.join(all_summaries) if all_summaries else '',\n",
        "        'key_points': all_points,\n",
        "        'decisions': all_decisions,\n",
        "        'action_items': all_actions\n",
        "    }\n",
        "\n",
        "    # Apply schema validation\n",
        "    merged = validate_schema(merged)\n",
        "\n",
        "    # If LLM failed catastrophically, use rule-based fallback\n",
        "    if (len(merged['action_items']) < 2 or\n",
        "        len(merged['key_points']) == 0 or\n",
        "        any(len(p) > 100 for p in merged['key_points'])):\n",
        "\n",
        "        print(\"    ‚ö†Ô∏è  LLM output quality too low, using rule-based extraction...\")\n",
        "        rule_based = rule_based_extraction(original_text)\n",
        "\n",
        "        # Merge LLM + rules (take best of both)\n",
        "        merged['action_items'] = rule_based['action_items'] if len(rule_based['action_items']) > len(merged['action_items']) else merged['action_items']\n",
        "        merged['decisions'] = rule_based['decisions'] if len(rule_based['decisions']) > len(merged['decisions']) else merged['decisions']\n",
        "        merged['key_points'] = rule_based['key_points']\n",
        "        merged['summary'] = rule_based['summary'] if len(rule_based['summary']) > len(merged['summary']) else merged['summary']\n",
        "\n",
        "    return merged\n",
        "\n",
        "\n",
        "print(\"‚úì Processing functions loaded with aggressive validation\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCBTEsyTrFlO",
        "outputId": "264918df-3dc4-47fb-9ab3-4ccc181fac18"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Processing functions loaded with aggressive validation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 7: MAIN PIPELINE (UPDATED)\n",
        "# ============================================================================\n",
        "\n",
        "def process_transcript(transcript_path: str) -> str:\n",
        "    \"\"\"Main pipeline with original text preservation\"\"\"\n",
        "\n",
        "    # Load transcript\n",
        "    with open(transcript_path, 'r', encoding='utf-8') as f:\n",
        "        raw_text = f.read()\n",
        "\n",
        "    print(f\"‚úì Loaded transcript: {len(raw_text)} characters\")\n",
        "\n",
        "    # Preprocess\n",
        "    clean_text = preprocess_transcript(raw_text)\n",
        "    print(f\"‚úì Preprocessed: {len(clean_text)} characters\")\n",
        "\n",
        "    # Chunk\n",
        "    chunks = chunk_text(clean_text, config.chunk_size, config.chunk_overlap)\n",
        "\n",
        "    print(f\"\\nüìä Processing {len(chunks)} chunk(s)...\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Map phase\n",
        "    chunk_results = []\n",
        "    for i, chunk in enumerate(chunks, 1):\n",
        "        print(f\"  [{i}/{len(chunks)}] Processing chunk ({len(chunk)} chars)...\", end=' ')\n",
        "        result = process_chunk(chunk)\n",
        "        chunk_results.append(result)\n",
        "        print(\"‚úì\")\n",
        "\n",
        "    # Reduce phase - pass original text for post-processing\n",
        "    print(\"-\" * 50)\n",
        "    print(\"üîÑ Merging results...\", end=' ')\n",
        "    final_result = merge_results(chunk_results, raw_text)  # Pass raw_text\n",
        "    print(\"‚úì\")\n",
        "\n",
        "    # Format as JSON\n",
        "    output_json = json.dumps(final_result, indent=2, ensure_ascii=False)\n",
        "\n",
        "    return output_json\n",
        "\n",
        "print(\"‚úì Main pipeline loaded\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M865g38urV0B",
        "outputId": "afa12680-e324-4dac-c48e-8b4620555964"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Main pipeline loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def check_file_exists(filename):\n",
        "    \"\"\"Helper to check if uploaded file exists\"\"\"\n",
        "    if os.path.exists(filename):\n",
        "        file_size = os.path.getsize(filename)\n",
        "\n",
        "        # Read first few lines to preview\n",
        "        with open(filename, 'r', encoding='utf-8') as f:\n",
        "            preview = f.read(200)\n",
        "\n",
        "        print(f\"‚úì Found: {filename} ({file_size:,} bytes)\")\n",
        "        print()\n",
        "        print(\"üìÑ Preview (first 200 chars):\")\n",
        "        print(\"-\" * 70)\n",
        "        print(preview + \"...\")\n",
        "        print(\"-\" * 70)\n",
        "        return True\n",
        "    else:\n",
        "        print(f\"‚úó Not found: {filename}\")\n",
        "        print(f\"  Please upload the file using the instructions above\")\n",
        "        return False"
      ],
      "metadata": {
        "id": "x_7yU4qwtFDg"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRANSCRIPT_FILE = 'meeting.txt'"
      ],
      "metadata": {
        "id": "dUXYPMtirftq"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print()\n",
        "if check_file_exists(TRANSCRIPT_FILE):\n",
        "    print(\"\\n‚úÖ File detected and ready!\")\n",
        "    print(\"üëâ Run Cell 9 to start processing\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è  File not found!\")\n",
        "    print(\"üëâ Please upload your file first, then run this cell again\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5nR4YOgslko",
        "outputId": "76128808-c561-4765-fb42-4bb2a9bcc6c7"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úì Found: meeting.txt (1,273 bytes)\n",
            "\n",
            "üìÑ Preview (first 200 chars):\n",
            "----------------------------------------------------------------------\n",
            "[00:00:03] John: Uh, okay, let's get started. So yeah, today's meeting is mainly about the Q2 product roadmap.\n",
            "\n",
            "[00:00:12] Sarah: Right, um, we need to finalize the timeline for the mobile app release...\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "‚úÖ File detected and ready!\n",
            "üëâ Run Cell 9 to start processing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 9: PROCESS YOUR TRANSCRIPT\n",
        "# ============================================================================\n",
        "# Process your uploaded meeting.txt file\n",
        "\n",
        "import os\n",
        "import time\n",
        "\n",
        "# Check if file was uploaded\n",
        "if not os.path.exists(TRANSCRIPT_FILE):\n",
        "    print(\"=\" * 70)\n",
        "    print(\"‚ùå ERROR: Transcript file not found!\")\n",
        "    print(\"=\" * 70)\n",
        "    print()\n",
        "    print(f\"Looking for: {TRANSCRIPT_FILE}\")\n",
        "    print()\n",
        "    print(\"Please:\")\n",
        "    print(\"1. Go back to Cell 8\")\n",
        "    print(\"2. Follow the upload instructions\")\n",
        "    print(\"3. Run Cell 8 again to verify upload\")\n",
        "    print(\"4. Then run this cell\")\n",
        "    print()\n",
        "    print(\"=\" * 70)\n",
        "else:\n",
        "    print(\"=\" * 70)\n",
        "    print(\"ü§ñ AI MEETING ASSISTANT - PROCESSING YOUR TRANSCRIPT\")\n",
        "    print(\"=\" * 70)\n",
        "    print()\n",
        "    print(f\"üìÑ Input File: {TRANSCRIPT_FILE}\")\n",
        "\n",
        "    # Show file info\n",
        "    file_size = os.path.getsize(TRANSCRIPT_FILE)\n",
        "    with open(TRANSCRIPT_FILE, 'r', encoding='utf-8') as f:\n",
        "        content = f.read()\n",
        "        line_count = len(content.split('\\n'))\n",
        "        word_count = len(content.split())\n",
        "\n",
        "    print(f\"üìä File Stats:\")\n",
        "    print(f\"   ‚Ä¢ Size: {file_size:,} bytes\")\n",
        "    print(f\"   ‚Ä¢ Lines: {line_count:,}\")\n",
        "    print(f\"   ‚Ä¢ Words: {word_count:,}\")\n",
        "    print()\n",
        "\n",
        "    # Start processing with timer\n",
        "    start_time = time.time()\n",
        "\n",
        "    print(\"‚è≥ Starting processing pipeline...\")\n",
        "    print()\n",
        "\n",
        "    # Process transcript\n",
        "    result_json = process_transcript(TRANSCRIPT_FILE)\n",
        "\n",
        "    # Calculate processing time\n",
        "    elapsed_time = time.time() - start_time\n",
        "    minutes = int(elapsed_time // 60)\n",
        "    seconds = int(elapsed_time % 60)\n",
        "\n",
        "    print()\n",
        "    print(\"=\" * 70)\n",
        "    print(\"‚úÖ PROCESSING COMPLETE!\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"‚è±Ô∏è  Processing Time: {minutes}m {seconds}s\")\n",
        "    print()\n",
        "    print(\"=\" * 70)\n",
        "    print(\"üìä RAW JSON OUTPUT\")\n",
        "    print(\"=\" * 70)\n",
        "    print(result_json)\n",
        "\n",
        "    # Store result for next cell\n",
        "    PROCESSED_RESULT = result_json\n",
        "\n",
        "    print()\n",
        "    print(\"=\" * 70)\n",
        "    print(\"üëâ Run Cell 10 to see formatted output\")\n",
        "    print(\"=\" * 70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMlSZn45s9tv",
        "outputId": "3e5008d2-03d4-48a1-b758-94601fafcfe0"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "ü§ñ AI MEETING ASSISTANT - PROCESSING YOUR TRANSCRIPT\n",
            "======================================================================\n",
            "\n",
            "üìÑ Input File: meeting.txt\n",
            "üìä File Stats:\n",
            "   ‚Ä¢ Size: 1,273 bytes\n",
            "   ‚Ä¢ Lines: 26\n",
            "   ‚Ä¢ Words: 199\n",
            "\n",
            "‚è≥ Starting processing pipeline...\n",
            "\n",
            "‚úì Loaded transcript: 1240 characters\n",
            "‚úì Preprocessed: 1088 characters\n",
            "\n",
            "üìä Processing 1 chunk(s)...\n",
            "--------------------------------------------------\n",
            "  [1/1] Processing chunk (1088 chars)... ‚úì\n",
            "--------------------------------------------------\n",
            "üîÑ Merging results...     ‚ö†Ô∏è  LLM output quality too low, using rule-based extraction...\n",
            "‚úì\n",
            "\n",
            "======================================================================\n",
            "‚úÖ PROCESSING COMPLETE!\n",
            "======================================================================\n",
            "‚è±Ô∏è  Processing Time: 9m 44s\n",
            "\n",
            "======================================================================\n",
            "üìä RAW JSON OUTPUT\n",
            "======================================================================\n",
            "{\n",
            "  \"summary\": \"The meeting focused on the Q2 product roadmap and the mobile app release timeline. Due to pending backend load testing, the team decided to delay the release by two weeks. Customer feedback highlighted issues with the onboarding experience, prompting discussion on simplification. Responsibilities were assigned to address onboarding improvements and coordinate with the design team.\",\n",
            "  \"key_points\": [\n",
            "    \"Q2 product roadmap discussion\",\n",
            "    \"Mobile app release timeline affected by backend load testing\",\n",
            "    \"Customer feedback indicating onboarding issues\",\n",
            "    \"Need for onboarding simplification and possible tutorials\",\n",
            "    \"Coordination required with the design team\"\n",
            "  ],\n",
            "  \"decisions\": [\n",
            "    \"The mobile app release will be delayed by two weeks.\"\n",
            "  ],\n",
            "  \"action_items\": [\n",
            "    {\n",
            "      \"task\": \"take ownership of improving the onboarding flow\",\n",
            "      \"owner\": \"Rahul\",\n",
            "      \"deadline\": null\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "\n",
            "======================================================================\n",
            "üëâ Run Cell 10 to see formatted output\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 10: DISPLAY FORMATTED OUTPUT\n",
        "# ============================================================================\n",
        "# Parse JSON and display in human-readable format\n",
        "\n",
        "import os\n",
        "\n",
        "if not os.path.exists(TRANSCRIPT_FILE):\n",
        "    print(\"‚ö†Ô∏è  Please run Cell 8 and Cell 9 first!\")\n",
        "else:\n",
        "    try:\n",
        "        result = json.loads(PROCESSED_RESULT)\n",
        "\n",
        "        print()\n",
        "        print(\"‚ïî\" + \"‚ïê\" * 68 + \"‚ïó\")\n",
        "        print(\"‚ïë\" + \" \" * 20 + \"MEETING ANALYSIS REPORT\" + \" \" * 25 + \"‚ïë\")\n",
        "        print(\"‚ïö\" + \"‚ïê\" * 68 + \"‚ïù\")\n",
        "        print()\n",
        "\n",
        "        # Summary Section\n",
        "        print(\"‚îå‚îÄ üìù SUMMARY \" + \"‚îÄ\" * 55 + \"‚îê\")\n",
        "        print(\"‚îÇ\" + \" \" * 68 + \"‚îÇ\")\n",
        "\n",
        "        # Word wrap the summary\n",
        "        summary = result['summary']\n",
        "        words = summary.split()\n",
        "        lines = []\n",
        "        current_line = \"\"\n",
        "\n",
        "        for word in words:\n",
        "            if len(current_line) + len(word) + 1 <= 64:\n",
        "                current_line += word + \" \"\n",
        "            else:\n",
        "                lines.append(current_line.strip())\n",
        "                current_line = word + \" \"\n",
        "        if current_line:\n",
        "            lines.append(current_line.strip())\n",
        "\n",
        "        for line in lines:\n",
        "            print(f\"‚îÇ  {line:<66}‚îÇ\")\n",
        "\n",
        "        print(\"‚îÇ\" + \" \" * 68 + \"‚îÇ\")\n",
        "        print(\"‚îî\" + \"‚îÄ\" * 68 + \"‚îò\")\n",
        "        print()\n",
        "\n",
        "        # Key Points Section\n",
        "        print(\"‚îå‚îÄ üí° KEY DISCUSSION POINTS \" + \"‚îÄ\" * 40 + \"‚îê\")\n",
        "        if result['key_points']:\n",
        "            for i, point in enumerate(result['key_points'], 1):\n",
        "                # Word wrap long points\n",
        "                if len(point) > 62:\n",
        "                    words = point.split()\n",
        "                    current = \"\"\n",
        "                    first_line = True\n",
        "                    for word in words:\n",
        "                        if len(current) + len(word) + 1 <= 62:\n",
        "                            current += word + \" \"\n",
        "                        else:\n",
        "                            if first_line:\n",
        "                                print(f\"‚îÇ  {i:2d}. {current.strip():<60}‚îÇ\")\n",
        "                                first_line = False\n",
        "                            else:\n",
        "                                print(f\"‚îÇ      {current.strip():<60}‚îÇ\")\n",
        "                            current = word + \" \"\n",
        "                    if current:\n",
        "                        if first_line:\n",
        "                            print(f\"‚îÇ  {i:2d}. {current.strip():<60}‚îÇ\")\n",
        "                        else:\n",
        "                            print(f\"‚îÇ      {current.strip():<60}‚îÇ\")\n",
        "                else:\n",
        "                    print(f\"‚îÇ  {i:2d}. {point:<62}‚îÇ\")\n",
        "        else:\n",
        "            print(\"‚îÇ      (No key points detected)\" + \" \" * 35 + \"‚îÇ\")\n",
        "        print(\"‚îî\" + \"‚îÄ\" * 68 + \"‚îò\")\n",
        "        print()\n",
        "\n",
        "        # Decisions Section\n",
        "        print(\"‚îå‚îÄ ‚úÖ DECISIONS MADE \" + \"‚îÄ\" * 47 + \"‚îê\")\n",
        "        if result['decisions']:\n",
        "            for i, decision in enumerate(result['decisions'], 1):\n",
        "                if len(decision) > 62:\n",
        "                    words = decision.split()\n",
        "                    current = \"\"\n",
        "                    first_line = True\n",
        "                    for word in words:\n",
        "                        if len(current) + len(word) + 1 <= 62:\n",
        "                            current += word + \" \"\n",
        "                        else:\n",
        "                            if first_line:\n",
        "                                print(f\"‚îÇ  {i:2d}. {current.strip():<60}‚îÇ\")\n",
        "                                first_line = False\n",
        "                            else:\n",
        "                                print(f\"‚îÇ      {current.strip():<60}‚îÇ\")\n",
        "                            current = word + \" \"\n",
        "                    if current:\n",
        "                        if first_line:\n",
        "                            print(f\"‚îÇ  {i:2d}. {current.strip():<60}‚îÇ\")\n",
        "                        else:\n",
        "                            print(f\"‚îÇ      {current.strip():<60}‚îÇ\")\n",
        "                else:\n",
        "                    print(f\"‚îÇ  {i:2d}. {decision:<62}‚îÇ\")\n",
        "        else:\n",
        "            print(\"‚îÇ      (No explicit decisions detected)\" + \" \" * 27 + \"‚îÇ\")\n",
        "        print(\"‚îî\" + \"‚îÄ\" * 68 + \"‚îò\")\n",
        "        print()\n",
        "\n",
        "        # Action Items Section\n",
        "        print(\"‚îå‚îÄ üìã ACTION ITEMS \" + \"‚îÄ\" * 49 + \"‚îê\")\n",
        "        if result['action_items']:\n",
        "            for i, item in enumerate(result['action_items'], 1):\n",
        "                task = item['task']\n",
        "                owner = item['owner'] if item['owner'] else '‚ö†Ô∏è  Unassigned'\n",
        "                deadline = item['deadline'] if item['deadline'] else '‚ö†Ô∏è  No deadline'\n",
        "\n",
        "                # Print task\n",
        "                if len(task) > 62:\n",
        "                    words = task.split()\n",
        "                    current = \"\"\n",
        "                    first_line = True\n",
        "                    for word in words:\n",
        "                        if len(current) + len(word) + 1 <= 62:\n",
        "                            current += word + \" \"\n",
        "                        else:\n",
        "                            if first_line:\n",
        "                                print(f\"‚îÇ  {i:2d}. {current.strip():<60}‚îÇ\")\n",
        "                                first_line = False\n",
        "                            else:\n",
        "                                print(f\"‚îÇ      {current.strip():<60}‚îÇ\")\n",
        "                            current = word + \" \"\n",
        "                    if current:\n",
        "                        if first_line:\n",
        "                            print(f\"‚îÇ  {i:2d}. {current.strip():<60}‚îÇ\")\n",
        "                        else:\n",
        "                            print(f\"‚îÇ      {current.strip():<60}‚îÇ\")\n",
        "                else:\n",
        "                    print(f\"‚îÇ  {i:2d}. {task:<62}‚îÇ\")\n",
        "\n",
        "                # Print owner and deadline\n",
        "                print(f\"‚îÇ      üë§ Owner: {owner:<51}‚îÇ\")\n",
        "                print(f\"‚îÇ      üìÖ Deadline: {deadline:<48}‚îÇ\")\n",
        "\n",
        "                # Add separator between items\n",
        "                if i < len(result['action_items']):\n",
        "                    print(\"‚îÇ      \" + \"‚îÄ\" * 60 + \"  ‚îÇ\")\n",
        "        else:\n",
        "            print(\"‚îÇ      (No action items detected)\" + \" \" * 33 + \"‚îÇ\")\n",
        "        print(\"‚îî\" + \"‚îÄ\" * 68 + \"‚îò\")\n",
        "        print()\n",
        "\n",
        "        # Summary Statistics\n",
        "        print(\"‚îå‚îÄ üìä STATISTICS \" + \"‚îÄ\" * 51 + \"‚îê\")\n",
        "        print(f\"‚îÇ  ‚Ä¢ Key Points Found: {len(result['key_points']):<44}‚îÇ\")\n",
        "        print(f\"‚îÇ  ‚Ä¢ Decisions Made: {len(result['decisions']):<46}‚îÇ\")\n",
        "        print(f\"‚îÇ  ‚Ä¢ Action Items: {len(result['action_items']):<48}‚îÇ\")\n",
        "\n",
        "        unassigned = sum(1 for item in result['action_items'] if not item['owner'])\n",
        "        no_deadline = sum(1 for item in result['action_items'] if not item['deadline'])\n",
        "\n",
        "        print(f\"‚îÇ  ‚Ä¢ Unassigned Actions: {unassigned:<42}‚îÇ\")\n",
        "        print(f\"‚îÇ  ‚Ä¢ Missing Deadlines: {no_deadline:<43}‚îÇ\")\n",
        "        print(\"‚îî\" + \"‚îÄ\" * 68 + \"‚îò\")\n",
        "        print()\n",
        "\n",
        "        # Warnings\n",
        "        if unassigned > 0 or no_deadline > 0:\n",
        "            print(\"‚ö†Ô∏è  ATTENTION REQUIRED:\")\n",
        "            if unassigned > 0:\n",
        "                print(f\"   ‚Ä¢ {unassigned} action item(s) need owner assignment\")\n",
        "            if no_deadline > 0:\n",
        "                print(f\"   ‚Ä¢ {no_deadline} action item(s) need deadline\")\n",
        "            print()\n",
        "\n",
        "        print(\"=\" * 70)\n",
        "        print(\"‚úÖ Analysis Complete! Run Cell 11 to save results.\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "    except NameError:\n",
        "        print(\"‚ö†Ô∏è  Please run Cell 9 first to process the transcript!\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error displaying results: {e}\")\n",
        "        print(\"Raw JSON is available in PROCESSED_RESULT variable\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TISk8tJstOED",
        "outputId": "d8ab1b32-23bf-4312-91ff-d287fbbd6400"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
            "‚ïë                    MEETING ANALYSIS REPORT                         ‚ïë\n",
            "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
            "\n",
            "‚îå‚îÄ üìù SUMMARY ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
            "‚îÇ                                                                    ‚îÇ\n",
            "‚îÇ  The meeting focused on the Q2 product roadmap and the mobile      ‚îÇ\n",
            "‚îÇ  app release timeline. Due to pending backend load testing, the    ‚îÇ\n",
            "‚îÇ  team decided to delay the release by two weeks. Customer          ‚îÇ\n",
            "‚îÇ  feedback highlighted issues with the onboarding experience,       ‚îÇ\n",
            "‚îÇ  prompting discussion on simplification. Responsibilities were     ‚îÇ\n",
            "‚îÇ  assigned to address onboarding improvements and coordinate with   ‚îÇ\n",
            "‚îÇ  the design team.                                                  ‚îÇ\n",
            "‚îÇ                                                                    ‚îÇ\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
            "\n",
            "‚îå‚îÄ üí° KEY DISCUSSION POINTS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
            "‚îÇ   1. Q2 product roadmap discussion                                 ‚îÇ\n",
            "‚îÇ   2. Mobile app release timeline affected by backend load testing  ‚îÇ\n",
            "‚îÇ   3. Customer feedback indicating onboarding issues                ‚îÇ\n",
            "‚îÇ   4. Need for onboarding simplification and possible tutorials     ‚îÇ\n",
            "‚îÇ   5. Coordination required with the design team                    ‚îÇ\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
            "\n",
            "‚îå‚îÄ ‚úÖ DECISIONS MADE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
            "‚îÇ   1. The mobile app release will be delayed by two weeks.          ‚îÇ\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
            "\n",
            "‚îå‚îÄ üìã ACTION ITEMS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
            "‚îÇ   1. take ownership of improving the onboarding flow               ‚îÇ\n",
            "‚îÇ      üë§ Owner: Rahul                                              ‚îÇ\n",
            "‚îÇ      üìÖ Deadline: ‚ö†Ô∏è  No deadline                                 ‚îÇ\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
            "\n",
            "‚îå‚îÄ üìä STATISTICS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
            "‚îÇ  ‚Ä¢ Key Points Found: 5                                           ‚îÇ\n",
            "‚îÇ  ‚Ä¢ Decisions Made: 1                                             ‚îÇ\n",
            "‚îÇ  ‚Ä¢ Action Items: 1                                               ‚îÇ\n",
            "‚îÇ  ‚Ä¢ Unassigned Actions: 0                                         ‚îÇ\n",
            "‚îÇ  ‚Ä¢ Missing Deadlines: 1                                          ‚îÇ\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
            "\n",
            "‚ö†Ô∏è  ATTENTION REQUIRED:\n",
            "   ‚Ä¢ 1 action item(s) need deadline\n",
            "\n",
            "======================================================================\n",
            "‚úÖ Analysis Complete! Run Cell 11 to save results.\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 11: SAVE OUTPUT TO FILE\n",
        "# ============================================================================\n",
        "# Save the JSON output to a file for download\n",
        "\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "if not os.path.exists(TRANSCRIPT_FILE):\n",
        "    print(\"‚ö†Ô∏è  Please run Cell 8 and Cell 9 first!\")\n",
        "else:\n",
        "    try:\n",
        "        # Generate timestamp for filenames\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "        # Save JSON output\n",
        "        json_filename = f'meeting_analysis_{timestamp}.json'\n",
        "        with open(json_filename, 'w', encoding='utf-8') as f:\n",
        "            f.write(PROCESSED_RESULT)\n",
        "\n",
        "        print(\"=\" * 70)\n",
        "        print(\"üíæ OUTPUTS SAVED SUCCESSFULLY\")\n",
        "        print(\"=\" * 70)\n",
        "        print()\n",
        "        print(f\"‚úì JSON File: {json_filename}\")\n",
        "\n",
        "        # Create detailed text report\n",
        "        result = json.loads(PROCESSED_RESULT)\n",
        "        txt_filename = f'meeting_analysis_{timestamp}.txt'\n",
        "\n",
        "        with open(txt_filename, 'w', encoding='utf-8') as f:\n",
        "            f.write(\"=\" * 70 + \"\\n\")\n",
        "            f.write(\"MEETING ANALYSIS REPORT\\n\")\n",
        "            f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "            f.write(\"=\" * 70 + \"\\n\\n\")\n",
        "\n",
        "            f.write(\"SUMMARY:\\n\")\n",
        "            f.write(\"-\" * 70 + \"\\n\")\n",
        "            f.write(f\"{result['summary']}\\n\\n\")\n",
        "\n",
        "            f.write(\"KEY DISCUSSION POINTS:\\n\")\n",
        "            f.write(\"-\" * 70 + \"\\n\")\n",
        "            if result['key_points']:\n",
        "                for i, point in enumerate(result['key_points'], 1):\n",
        "                    f.write(f\"{i}. {point}\\n\")\n",
        "            else:\n",
        "                f.write(\"(No key points detected)\\n\")\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "            f.write(\"DECISIONS MADE:\\n\")\n",
        "            f.write(\"-\" * 70 + \"\\n\")\n",
        "            if result['decisions']:\n",
        "                for i, decision in enumerate(result['decisions'], 1):\n",
        "                    f.write(f\"{i}. {decision}\\n\")\n",
        "            else:\n",
        "                f.write(\"(No explicit decisions detected)\\n\")\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "            f.write(\"ACTION ITEMS:\\n\")\n",
        "            f.write(\"-\" * 70 + \"\\n\")\n",
        "            if result['action_items']:\n",
        "                for i, item in enumerate(result['action_items'], 1):\n",
        "                    owner = item['owner'] if item['owner'] else 'Unassigned'\n",
        "                    deadline = item['deadline'] if item['deadline'] else 'No deadline'\n",
        "                    f.write(f\"\\n{i}. {item['task']}\\n\")\n",
        "                    f.write(f\"   Owner: {owner}\\n\")\n",
        "                    f.write(f\"   Deadline: {deadline}\\n\")\n",
        "            else:\n",
        "                f.write(\"(No action items detected)\\n\")\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "            # Add statistics\n",
        "            f.write(\"=\" * 70 + \"\\n\")\n",
        "            f.write(\"STATISTICS:\\n\")\n",
        "            f.write(\"-\" * 70 + \"\\n\")\n",
        "            f.write(f\"Key Points: {len(result['key_points'])}\\n\")\n",
        "            f.write(f\"Decisions: {len(result['decisions'])}\\n\")\n",
        "            f.write(f\"Action Items: {len(result['action_items'])}\\n\")\n",
        "\n",
        "            unassigned = sum(1 for item in result['action_items'] if not item['owner'])\n",
        "            no_deadline = sum(1 for item in result['action_items'] if not item['deadline'])\n",
        "\n",
        "            f.write(f\"Unassigned Actions: {unassigned}\\n\")\n",
        "            f.write(f\"Missing Deadlines: {no_deadline}\\n\")\n",
        "            f.write(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "        print(f\"‚úì Text Report: {txt_filename}\")\n",
        "        print()\n",
        "\n",
        "        # Create CSV for action items (if any)\n",
        "        if result['action_items']:\n",
        "            csv_filename = f'action_items_{timestamp}.csv'\n",
        "            with open(csv_filename, 'w', encoding='utf-8') as f:\n",
        "                f.write(\"ID,Task,Owner,Deadline,Status\\n\")\n",
        "                for i, item in enumerate(result['action_items'], 1):\n",
        "                    task = item['task'].replace('\"', '\"\"')  # Escape quotes\n",
        "                    owner = item['owner'] if item['owner'] else 'Unassigned'\n",
        "                    deadline = item['deadline'] if item['deadline'] else 'No deadline'\n",
        "                    status = 'Pending'\n",
        "                    f.write(f'{i},\"{task}\",\"{owner}\",\"{deadline}\",\"{status}\"\\n')\n",
        "\n",
        "            print(f\"‚úì Action Items CSV: {csv_filename}\")\n",
        "            print()\n",
        "\n",
        "        print(\"=\" * 70)\n",
        "        print(\"üì• TO DOWNLOAD FILES:\")\n",
        "        print(\"=\" * 70)\n",
        "        print(\"1. Click the üìÅ FOLDER icon in the left sidebar\")\n",
        "        print(\"2. Find the files listed above\")\n",
        "        print(\"3. Click ‚ãÆ (three dots) next to each filename\")\n",
        "        print(\"4. Select 'Download'\")\n",
        "        print()\n",
        "        print(\"FILES GENERATED:\")\n",
        "        print(f\"  üìÑ {json_filename} - Machine-readable JSON\")\n",
        "        print(f\"  üìÑ {txt_filename} - Human-readable report\")\n",
        "        if result['action_items']:\n",
        "            print(f\"  üìÑ {csv_filename} - Action items spreadsheet\")\n",
        "        print()\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "    except NameError:\n",
        "        print(\"‚ö†Ô∏è  Please run Cell 9 first to process the transcript!\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error saving files: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0r0zTs_uIwa",
        "outputId": "325437d8-05e2-4ea3-996c-1939b9778a80"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "üíæ OUTPUTS SAVED SUCCESSFULLY\n",
            "======================================================================\n",
            "\n",
            "‚úì JSON File: meeting_analysis_20260103_084248.json\n",
            "‚úì Text Report: meeting_analysis_20260103_084248.txt\n",
            "\n",
            "‚úì Action Items CSV: action_items_20260103_084248.csv\n",
            "\n",
            "======================================================================\n",
            "üì• TO DOWNLOAD FILES:\n",
            "======================================================================\n",
            "1. Click the üìÅ FOLDER icon in the left sidebar\n",
            "2. Find the files listed above\n",
            "3. Click ‚ãÆ (three dots) next to each filename\n",
            "4. Select 'Download'\n",
            "\n",
            "FILES GENERATED:\n",
            "  üìÑ meeting_analysis_20260103_084248.json - Machine-readable JSON\n",
            "  üìÑ meeting_analysis_20260103_084248.txt - Human-readable report\n",
            "  üìÑ action_items_20260103_084248.csv - Action items spreadsheet\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I4kYb661uTXv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}